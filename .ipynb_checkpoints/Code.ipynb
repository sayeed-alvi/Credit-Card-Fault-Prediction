{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"D:\\\\Himanshu\\\\Acads\\\\02. Spring semester\\\\02. Stat 656\\\\Prof. Jones Modules\")\n",
    "\n",
    "# ! conda install -c glemaitre imbalanced-learn\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from Class_replace_impute_encode import ReplaceImputeEncode\n",
    "from Class_regression_1 import logreg\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Loss as shown by Prof. Jones\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for calculating loss and confusion matrix\n",
    "def loss_cal(lg, X, y, fp_cost, fn_cost):\n",
    "    loss     = [0, 0]       #False Neg Cost, False Pos Cost\n",
    "    conf_mat = [0, 0, 0, 0] #tn, fp, fn, tp\n",
    "    predictions = lg.predict(X)\n",
    "    for j in range(len(y)):\n",
    "        if y[j]==0:\n",
    "            if predictions[j]==0:\n",
    "                conf_mat[0] += 1 #True Negative\n",
    "            else:\n",
    "                conf_mat[1] += 1 #False Positive\n",
    "                loss[1] += fp_cost[j]\n",
    "        else:\n",
    "            if predictions[j]==1:\n",
    "                conf_mat[3] += 1 #True Positive\n",
    "            else:\n",
    "                conf_mat[2] += 1 #False Negative\n",
    "                loss[0] += fn_cost[j]\n",
    "    return loss, conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Attribute Map for CreditData_RareEvent.xlsx, N=10,500\n",
    "attribute_map = {\n",
    "    'age':[0,(1, 120),[0,0]],\n",
    "    'amount':[0,(0, 20000),[0,0]],\n",
    "    'duration':[0,(1,100),[0,0]],\n",
    "    'checking':[2,(1, 2, 3, 4),[0,0]],\n",
    "    'coapp':[2,(1,2,3),[0,0]],\n",
    "    'depends':[1,(1,2),[0,0]],\n",
    "    'employed':[2,(1,2,3,4,5),[0,0]],\n",
    "    'existcr':[2,(1,2,3,4),[0,0]],\n",
    "    'foreign':[1,(1,2),[0,0]],\n",
    "    'good_bad':[1,('bad', 'good'),[0,0]],\n",
    "    'history':[2,(0,1,2,3,4),[0,0]],\n",
    "    'housing':[2,(1, 2, 3), [0,0]],\n",
    "    'installp':[2,(1,2,3,4),[0,0]],\n",
    "    'job':[2,(1,2,3,4),[0,0]],\n",
    "    'marital':[2,(1,2,3,4),[0,0]],\n",
    "    'other':[2,(1,2,3),[0,0]],\n",
    "    'property':[2,(1,2,3,4),[0,0]],\n",
    "    'resident':[2,(1,2,3,4),[0,0]],\n",
    "    'savings':[2,(1,2,3,4,5),[0,0]],\n",
    "    'telephon':[1,(1,2),[0,0]] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********** Data Preprocessing ***********\n",
      "Features Dictionary Contains:\n",
      "3 Interval, \n",
      "4 Binary, and \n",
      "13 Nominal Attribute(s).\n",
      "\n",
      "Data contains 10500 observations & 20 columns.\n",
      "\n",
      "\n",
      "Attribute Counts\n",
      "............... Missing  Outliers\n",
      "age.......         0         0\n",
      "amount....         0         0\n",
      "duration..         0         0\n",
      "checking..         0         0\n",
      "coapp.....         0         0\n",
      "depends...         0         0\n",
      "employed..         0         0\n",
      "existcr...         0         0\n",
      "foreign...         0         0\n",
      "good_bad..         0         0\n",
      "history...         0         0\n",
      "housing...         0         0\n",
      "installp..         0         0\n",
      "job.......         0         0\n",
      "marital...         0         0\n",
      "other.....         0         0\n",
      "property..         0         0\n",
      "resident..         0         0\n",
      "savings...         0         0\n",
      "telephon..         0         0\n",
      "\n",
      "Logistic Regression Model using Entire Dataset\n",
      "\n",
      "Coefficients:\n",
      "Intercept..         4.6940\n",
      "age........         0.0117\n",
      "amount.....        -0.0002\n",
      "duration...        -0.0185\n",
      "depends....        -0.3062\n",
      "foreign....         1.2001\n",
      "telephon...         0.2601\n",
      "checking0..        -1.6210\n",
      "checking1..        -1.2405\n",
      "checking2..        -0.6602\n",
      "coapp0.....        -0.8559\n",
      "coapp1.....        -1.0848\n",
      "employed0..        -0.3012\n",
      "employed1..        -0.5079\n",
      "employed2..        -0.2583\n",
      "employed3..         0.3510\n",
      "existcr0...         1.2302\n",
      "existcr1...         0.8801\n",
      "existcr2...         1.5614\n",
      "history0...        -1.6030\n",
      "history1...        -1.4763\n",
      "history2...        -0.6810\n",
      "history3...        -0.3551\n",
      "housing0...        -0.7103\n",
      "housing1...        -0.1830\n",
      "installp0..         1.1586\n",
      "installp1..         0.9359\n",
      "installp2..         0.4602\n",
      "job0.......         0.3046\n",
      "job1.......        -0.3355\n",
      "job2.......        -0.3164\n",
      "marital0...         0.1367\n",
      "marital1...        -0.1247\n",
      "marital2...         0.6070\n",
      "other0.....        -0.3451\n",
      "other1.....        -0.3730\n",
      "property0..         0.8421\n",
      "property1..         0.7429\n",
      "property2..         0.7118\n",
      "resident0..         0.7300\n",
      "resident1..        -0.3991\n",
      "resident2..         0.1047\n",
      "savings0...        -0.9106\n",
      "savings1...        -0.6388\n",
      "savings2...        -0.4378\n",
      "savings3...         1.0900\n",
      "\n",
      "Model Metrics\n",
      "Observations...............     10500\n",
      "Coefficients...............        46\n",
      "DF Error...................     10454\n",
      "Mean Absolute Error........    0.0793\n",
      "Avg Squared Error..........    0.0384\n",
      "Accuracy...................    0.9555\n",
      "Precision..................    0.9554\n",
      "Recall (Sensitivity).......    1.0000\n",
      "F1-Score...................    0.9772\n",
      "MISC (Misclassification)...      4.4%\n",
      "     class 0...............     93.4%\n",
      "     class 1...............      0.0%\n",
      "\n",
      "\n",
      "     Confusion\n",
      "       Matrix     Class 0   Class 1  \n",
      "Class 0.....        33       467\n",
      "Class 1.....         0     10000\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"D:\\\\Himanshu\\\\Acads\\\\02. Spring semester\\\\02. Stat 656\\\\Lectures\\\\Week-8 Modelling rare events\\\\CreditData_RareEvent.xlsx\", encoding='latin-1', error_bad_lines=False)\n",
    "\n",
    "# Use replace impute node to hot-encode the missing values and outlier values\n",
    "rie = ReplaceImputeEncode(data_map=attribute_map, nominal_encoding='one-hot', \\\n",
    "                          interval_scale = None, drop=True, display=True)\n",
    "encoded_df = rie.fit_transform(df)\n",
    "encoded_df.good_bad.value_counts()\n",
    "# Create X and y, numpy arrays\n",
    "\n",
    "encoded_df.good_bad.value_counts() # 1 is good and 0 is bad\n",
    "# That means we are giving higher cost to a person who is bad but categorized as good - bcz is it risky to give him loan\n",
    "\n",
    "\n",
    "y = np.asarray(encoded_df['good_bad']) # The target is not scaled or imputed\n",
    "X = np.asarray(encoded_df.drop('good_bad',axis=1))\n",
    "\n",
    "# Use cost in the data to get false positive and false negative cost\n",
    "\n",
    "fp_cost = np.array(df['amount'])\n",
    "fn_cost = np.array(0.1*df['amount'])\n",
    "\n",
    "lgr = LogisticRegression()\n",
    "lgr.fit(X, y)\n",
    "print(\"\\nLogistic Regression Model using Entire Dataset\")\n",
    "col = rie.col\n",
    "col.remove('good_bad')\n",
    "logreg.display_coef(lgr, X.shape[1], 2, col)\n",
    "logreg.display_binary_metrics(lgr, X, y) # 95% accuracy on training dataset # no False Negative but lot of False positive.. \n",
    "# probably the cutoff is at 0.5, and lot of bad customers are predicted as good - we need to minimize this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Model using 50:50 RUS\n",
      "[366, 134, 2945, 7055]\n",
      "[363, 137, 2614, 7386]\n",
      "[354, 146, 2974, 7026]\n",
      "[359, 141, 2553, 7447]\n",
      "[372, 128, 2710, 7290]\n",
      "[383, 117, 3006, 6994]\n",
      "[366, 134, 2914, 7086]\n",
      "[384, 116, 2820, 7180]\n",
      "[364, 136, 2722, 7278]\n",
      "[362, 138, 2765, 7235]\n",
      "Misclassification Rate.    0.2795\n",
      "False Negative Cost....   1057043\n",
      "False Positive Cost....    408676\n",
      "Total Loss.............   1465719 +/- 70908.05  \n",
      "\n",
      "Logistic Regression Model using 60:40 RUS\n",
      "[301, 199, 1819, 8181]\n",
      "[303, 197, 1848, 8152]\n",
      "[342, 158, 1898, 8102]\n",
      "[310, 190, 1822, 8178]\n",
      "[322, 178, 1997, 8003]\n",
      "[321, 179, 1948, 8052]\n",
      "[298, 202, 1889, 8111]\n",
      "[314, 186, 1946, 8054]\n",
      "[315, 185, 1897, 8103]\n",
      "[295, 205, 1668, 8332]\n",
      "Misclassification Rate.    0.1963\n",
      "False Negative Cost....    744295\n",
      "False Positive Cost....    557882\n",
      "Total Loss.............   1302177 +/- 39419.17  \n",
      "\n",
      "Logistic Regression Model using 70:30 RUS\n",
      "[265, 235, 1277, 8723]\n",
      "[266, 234, 1309, 8691]\n",
      "[264, 236, 1175, 8825]\n",
      "[263, 237, 1144, 8856]\n",
      "[271, 229, 1241, 8759]\n",
      "[255, 245, 1188, 8812]\n",
      "[261, 239, 1253, 8747]\n",
      "[254, 246, 1058, 8942]\n",
      "[264, 236, 1173, 8827]\n",
      "[241, 259, 1046, 8954]\n",
      "Misclassification Rate.    0.1358\n",
      "False Negative Cost....    497789\n",
      "False Positive Cost....    777656\n",
      "Total Loss.............   1275445 +/- 40559.50  \n",
      "\n",
      "Logistic Regression Model using 80:20 RUS\n",
      "[160, 340, 471, 9529]\n",
      "[192, 308, 543, 9457]\n",
      "[178, 322, 483, 9517]\n",
      "[192, 308, 572, 9428]\n",
      "[175, 325, 469, 9531]\n",
      "[184, 316, 623, 9377]\n",
      "[101, 399, 378, 9622]\n",
      "[192, 308, 528, 9472]\n",
      "[104, 396, 371, 9629]\n",
      "[202, 298, 593, 9407]\n",
      "Misclassification Rate.    0.0795\n",
      "False Negative Cost....    231609\n",
      "False Positive Cost....   1193956\n",
      "Total Loss.............   1425566 +/- 114026.29 \n",
      "\n",
      "Logistic Regression Model using 90:10 RUS\n",
      "[68, 432, 100, 9900]\n",
      "[72, 428, 91, 9909]\n",
      "[63, 437, 61, 9939]\n",
      "[75, 425, 93, 9907]\n",
      "[76, 424, 131, 9869]\n",
      "[56, 444, 114, 9886]\n",
      "[69, 431, 105, 9895]\n",
      "[75, 425, 84, 9916]\n",
      "[61, 439, 79, 9921]\n",
      "[66, 434, 84, 9916]\n",
      "Misclassification Rate.    0.0501\n",
      "False Negative Cost....     66550\n",
      "False Positive Cost....   1601043\n",
      "Total Loss.............   1667593 +/- 47502.60  \n"
     ]
    }
   ],
   "source": [
    "# Dbt: How do you decide that False Positive to be given more weight or False Negative in MLE??\n",
    "# Ratio of Good vs Bad in ratio list. Dictionary has Bad: Good values\n",
    "ratio = [ '50:50', '60:40', '70:30', '80:20', '90:10' ]\n",
    "# n_majority = ratio x n_minority\n",
    "rus_ratio = ({0:500, 1:500}, {0:500, 1:750}, {0:500, 1:1167}, {0:500, 1:2000},\\\n",
    "             {0:500, 1:4500})\n",
    "\n",
    "# Create 10 different samples for each ratio.\n",
    "# Create random seeds\n",
    "rand_val = np.array([1, 12, 123, 1234, 12345, 654321, 54321, 4321, 321, 21])\n",
    "\n",
    "# Best model is one that minimizes the loss\n",
    "min_loss   = 9e+15\n",
    "best_ratio = 0\n",
    "# \n",
    "for k in  range(len(rus_ratio)):\n",
    "    rand_vals = (k+1)*rand_val\n",
    "    print(\"\\nLogistic Regression Model using \" + ratio[k] + \" RUS\")\n",
    "    fn_loss = np.zeros(len(rand_vals))\n",
    "    fp_loss = np.zeros(len(rand_vals))\n",
    "    misc    = np.zeros(len(rand_vals))\n",
    "    for i in range(len(rand_vals)):\n",
    "        rus = RandomUnderSampler(ratio=rus_ratio[k], random_state=rand_vals[i], return_indices=False, replacement=False)\n",
    "        X_rus, Y_rus = rus.fit_sample(X,y)\n",
    "        lgr= LogisticRegression(C=1e+15, tol=1e-8, max_iter=300, solver='lbfgs')\n",
    "        lgr.fit(X_rus, Y_rus)\n",
    "        \n",
    "        ## interesing thing is, loss calculation would run on full dataset using fitting model of sample data. so prediction would happen on full dataset.\n",
    "        \n",
    "        loss, conf_mat = loss_cal(lgr, X, y, fp_cost, fn_cost)\n",
    "        fn_loss[i] = loss[0]\n",
    "        fp_loss[i] = loss[1]\n",
    "        misc[i] = conf_mat[1]+conf_mat[2]\n",
    "        print(conf_mat)\n",
    "        \n",
    "# calculate an average of missclassfication rate across all samples for one ratio type\n",
    "\n",
    "    misc = np.sum(misc)/(10500 * len(rand_vals)) # Why are we dividing by 10500? We should divide by total sample size of 10 samples \n",
    "\n",
    "    fn_avg_loss = np.average(fn_loss)\n",
    "    fp_avg_loss = np.average(fp_loss)\n",
    "\n",
    "    total_loss = fn_loss + fp_loss\n",
    "\n",
    "    avg_loss = np.average(total_loss)\n",
    "    std_loss = np.std(total_loss)\n",
    "\n",
    "    print(\"{:.<23s}{:10.4f}\".format(\"Misclassification Rate\", misc))\n",
    "    print(\"{:.<23s}{:10.0f}\".format(\"False Negative Cost\", fn_avg_loss))\n",
    "    print(\"{:.<23s}{:10.0f}\".format(\"False Positive Cost\", fp_avg_loss))\n",
    "    print(\"{:.<23s}{:10.0f}{:5s}{:<10.2f}\".format(\"Total Loss\", avg_loss, \" +/- \", std_loss))\n",
    "\n",
    "    # set a higher value for min_loss so that the function doesnt output error if min loss is higher than that\n",
    "\n",
    "    if avg_loss < min_loss:\n",
    "        min_loss = avg_loss\n",
    "        best_ratio = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Random Over sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "os = RandomOverSampler(ratio={0:10000, 1:10000}, random_state=1)\n",
    "X_ros, y_ros = os.fit_sample(X, y) # 5000 observation of minority class now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Model using 50:50 RUS\n",
      "[375, 125, 2763, 7237]\n",
      "[362, 138, 2775, 7225]\n",
      "[360, 140, 2722, 7278]\n",
      "[368, 132, 2718, 7282]\n",
      "[367, 133, 2656, 7344]\n",
      "[369, 131, 2792, 7208]\n",
      "[372, 128, 2754, 7246]\n",
      "[366, 134, 2635, 7365]\n",
      "[363, 137, 2700, 7300]\n",
      "[355, 145, 2636, 7364]\n",
      "Misclassification Rate.    0.2714\n",
      "False Negative Cost....   1016410\n",
      "False Positive Cost....    411619\n",
      "Total Loss.............   1428029 +/- 27663.60  \n",
      "\n",
      "Logistic Regression Model using 40:60 RUS\n",
      "[301, 199, 1796, 8204]\n",
      "[317, 183, 1822, 8178]\n",
      "[309, 191, 1813, 8187]\n",
      "[313, 187, 1798, 8202]\n",
      "[299, 201, 1825, 8175]\n",
      "[313, 187, 1835, 8165]\n",
      "[306, 194, 1793, 8207]\n",
      "[316, 184, 1817, 8183]\n",
      "[300, 200, 1725, 8275]\n",
      "[305, 195, 1824, 8176]\n",
      "Misclassification Rate.    0.1902\n",
      "False Negative Cost....    717044\n",
      "False Positive Cost....    571467\n",
      "Total Loss.............   1288511 +/- 39817.16  \n",
      "\n",
      "Logistic Regression Model using 30:70 RUS\n",
      "[264, 236, 1107, 8893]\n",
      "[246, 254, 969, 9031]\n",
      "[255, 245, 1084, 8916]\n",
      "[250, 250, 1118, 8882]\n",
      "[250, 250, 980, 9020]\n",
      "[267, 233, 1086, 8914]\n",
      "[242, 258, 1071, 8929]\n",
      "[259, 241, 1129, 8871]\n",
      "[261, 239, 1262, 8738]\n",
      "[247, 253, 1050, 8950]\n",
      "Misclassification Rate.    0.1268\n",
      "False Negative Cost....    479481\n",
      "False Positive Cost....    804500\n",
      "Total Loss.............   1283981 +/- 32189.41  \n",
      "\n",
      "Logistic Regression Model using 20:80 RUS\n",
      "[182, 318, 504, 9496]\n",
      "[184, 316, 526, 9474]\n",
      "[174, 326, 470, 9530]\n",
      "[181, 319, 470, 9530]\n",
      "[179, 321, 483, 9517]\n",
      "[170, 330, 543, 9457]\n",
      "[165, 335, 473, 9527]\n",
      "[172, 328, 495, 9505]\n",
      "[196, 304, 512, 9488]\n",
      "[155, 345, 361, 9639]\n",
      "Misclassification Rate.    0.0769\n",
      "False Negative Cost....    227076\n",
      "False Positive Cost....   1148598\n",
      "Total Loss.............   1375673 +/- 33615.63  \n",
      "\n",
      "Logistic Regression Model using 10:90 RUS\n",
      "[54, 446, 37, 9963]\n",
      "[67, 433, 84, 9916]\n",
      "[70, 430, 119, 9881]\n",
      "[58, 442, 61, 9939]\n",
      "[61, 439, 91, 9909]\n",
      "[61, 439, 76, 9924]\n",
      "[67, 433, 90, 9910]\n",
      "[59, 441, 76, 9924]\n",
      "[72, 428, 90, 9910]\n",
      "[70, 430, 91, 9909]\n",
      "Misclassification Rate.    0.0493\n",
      "False Negative Cost....     60860\n",
      "False Positive Cost....   1602983\n",
      "Total Loss.............   1663843 +/- 17286.35  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ratio of Good vs Bad in ratio list. Dictionary has Bad: Good values\n",
    "ratio = [ '50:50', '40:60', '30:70', '20:80', '10:90' ]\n",
    "# n_majority = ratio x n_minority\n",
    "rus_ratio = ({0:10000, 1:10000}, {0:6666, 1:10000}, {0:4286, 1:10000}, {0:2500, 1:10000},\\\n",
    "             {0:1111,  1:10000})\n",
    "\n",
    "# Create 10 different samples for each ratio.\n",
    "# Create random seeds\n",
    "rand_val = np.array([1, 12, 123, 1234, 12345, 654321, 54321, 4321, 321, 21])\n",
    "\n",
    "# Best model is one that minimizes the loss\n",
    "min_loss   = 9e+15\n",
    "best_ratio = 0\n",
    "# \n",
    "for k in  range(len(rus_ratio)):\n",
    "    rand_vals = (k+1)*rand_val\n",
    "    print(\"\\nLogistic Regression Model using \" + ratio[k] + \" RUS\")\n",
    "    fn_loss = np.zeros(len(rand_vals))\n",
    "    fp_loss = np.zeros(len(rand_vals))\n",
    "    misc    = np.zeros(len(rand_vals))\n",
    "    for i in range(len(rand_vals)):\n",
    "        ros = RandomOverSampler(ratio=rus_ratio[k], random_state=rand_vals[i])\n",
    "        X_ros, Y_ros = ros.fit_sample(X,y)\n",
    "        lgr= LogisticRegression(C=1e+15, tol=1e-8, max_iter=300, solver='lbfgs')\n",
    "        lgr.fit(X_ros, Y_ros)\n",
    "        \n",
    "        ## interesing thing is, loss calculation would run on full dataset using fitting model of sample data. so prediction would happen on full dataset.\n",
    "        \n",
    "        loss, conf_mat = loss_cal(lgr, X, y, fp_cost, fn_cost)\n",
    "        fn_loss[i] = loss[0]\n",
    "        fp_loss[i] = loss[1]\n",
    "        misc[i] = conf_mat[1]+conf_mat[2]\n",
    "        print(conf_mat)\n",
    "        \n",
    "# calculate an average of missclassfication rate across all samples for one ratio type\n",
    "\n",
    "    misc = np.sum(misc)/(10500 * len(rand_vals)) # Why are we dividing by 10500? We should divide by total sample size of 10 samples \n",
    "\n",
    "    fn_avg_loss = np.average(fn_loss)\n",
    "    fp_avg_loss = np.average(fp_loss)\n",
    "\n",
    "    total_loss = fn_loss + fp_loss\n",
    "\n",
    "    avg_loss = np.average(total_loss)\n",
    "    std_loss = np.std(total_loss)\n",
    "\n",
    "    print(\"{:.<23s}{:10.4f}\".format(\"Misclassification Rate\", misc))\n",
    "    print(\"{:.<23s}{:10.0f}\".format(\"False Negative Cost\", fn_avg_loss))\n",
    "    print(\"{:.<23s}{:10.0f}\".format(\"False Positive Cost\", fp_avg_loss))\n",
    "    print(\"{:.<23s}{:10.0f}{:5s}{:<10.2f}\".format(\"Total Loss\", avg_loss, \" +/- \", std_loss))\n",
    "\n",
    "    # set a higher value for min_loss so that the function doesnt output error if min loss is higher than that\n",
    "\n",
    "    if avg_loss < min_loss:\n",
    "        min_loss = avg_loss\n",
    "        best_ratio = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Model using 50:50 RUS\n",
      "[362, 138, 2518, 7482]\n",
      "[368, 132, 2709, 7291]\n",
      "[366, 134, 2553, 7447]\n",
      "[370, 130, 2635, 7365]\n",
      "[364, 136, 2595, 7405]\n",
      "[360, 140, 2514, 7486]\n",
      "[362, 138, 2493, 7507]\n",
      "[366, 134, 2669, 7331]\n",
      "[376, 124, 2627, 7373]\n",
      "[374, 126, 2557, 7443]\n",
      "Misclassification Rate.    0.2591\n",
      "False Negative Cost....    949419\n",
      "False Positive Cost....    414988\n",
      "Total Loss.............   1364407 +/- 21425.36  \n",
      "\n",
      "Logistic Regression Model using 40:60 RUS\n",
      "[309, 191, 2016, 7984]\n",
      "[306, 194, 1815, 8185]\n",
      "[312, 188, 1833, 8167]\n",
      "[305, 195, 1767, 8233]\n",
      "[318, 182, 1856, 8144]\n",
      "[309, 191, 1930, 8070]\n",
      "[316, 184, 1864, 8136]\n",
      "[311, 189, 1903, 8097]\n",
      "[314, 186, 1961, 8039]\n",
      "[317, 183, 1858, 8142]\n",
      "Misclassification Rate.    0.1970\n",
      "False Negative Cost....    722083\n",
      "False Positive Cost....    572992\n",
      "Total Loss.............   1295075 +/- 32088.32  \n",
      "\n",
      "Logistic Regression Model using 30:70 RUS\n",
      "[258, 242, 1107, 8893]\n",
      "[253, 247, 1190, 8810]\n",
      "[253, 247, 1193, 8807]\n",
      "[245, 255, 1056, 8944]\n",
      "[255, 245, 1123, 8877]\n",
      "[259, 241, 1187, 8813]\n",
      "[260, 240, 1193, 8807]\n",
      "[259, 241, 1168, 8832]\n",
      "[258, 242, 1255, 8745]\n",
      "[250, 250, 1172, 8828]\n",
      "Misclassification Rate.    0.1342\n",
      "False Negative Cost....    491860\n",
      "False Positive Cost....    827293\n",
      "Total Loss.............   1319153 +/- 34428.73  \n",
      "\n",
      "Logistic Regression Model using 20:80 RUS\n",
      "[200, 300, 552, 9448]\n",
      "[190, 310, 415, 9585]\n",
      "[200, 300, 559, 9441]\n",
      "[196, 304, 522, 9478]\n",
      "[210, 290, 558, 9442]\n",
      "[195, 305, 490, 9510]\n",
      "[209, 291, 551, 9449]\n",
      "[193, 307, 501, 9499]\n",
      "[184, 316, 539, 9461]\n",
      "[175, 325, 430, 9570]\n",
      "Misclassification Rate.    0.0778\n",
      "False Negative Cost....    231013\n",
      "False Positive Cost....   1102992\n",
      "Total Loss.............   1334005 +/- 34480.85  \n",
      "\n",
      "Logistic Regression Model using 10:90 RUS\n",
      "[67, 433, 125, 9875]\n",
      "[71, 429, 90, 9910]\n",
      "[25, 475, 47, 9953]\n",
      "[62, 438, 81, 9919]\n",
      "[69, 431, 105, 9895]\n",
      "[80, 420, 99, 9901]\n",
      "[69, 431, 90, 9910]\n",
      "[81, 419, 130, 9870]\n",
      "[76, 424, 148, 9852]\n",
      "[79, 421, 131, 9869]\n",
      "Misclassification Rate.    0.0511\n",
      "False Negative Cost....     69239\n",
      "False Positive Cost....   1607761\n",
      "Total Loss.............   1677000 +/- 84215.36  \n"
     ]
    }
   ],
   "source": [
    "# Using SMOTE--\n",
    "# Ratio of Good vs Bad in ratio list. Dictionary has Bad: Good values\n",
    "ratio = [ '50:50', '40:60', '30:70', '20:80', '10:90' ]\n",
    "# n_majority = ratio x n_minority\n",
    "rus_ratio = ({0:10000, 1:10000}, {0:6666, 1:10000}, {0:4286, 1:10000}, {0:2500, 1:10000},\\\n",
    "             {0:1111,  1:10000})\n",
    "\n",
    "# Create 10 different samples for each ratio.\n",
    "# Create random seeds\n",
    "rand_val = np.array([1, 12, 123, 1234, 12345, 654321, 54321, 4321, 321, 21])\n",
    "\n",
    "# Best model is one that minimizes the loss\n",
    "min_loss   = 9e+15\n",
    "best_ratio = 0\n",
    "# \n",
    "for k in  range(len(rus_ratio)):\n",
    "    rand_vals = (k+1)*rand_val\n",
    "    print(\"\\nLogistic Regression Model using \" + ratio[k] + \" RUS\")\n",
    "    fn_loss = np.zeros(len(rand_vals))\n",
    "    fp_loss = np.zeros(len(rand_vals))\n",
    "    misc    = np.zeros(len(rand_vals))\n",
    "    for i in range(len(rand_vals)):\n",
    "        ros = SMOTE(ratio=rus_ratio[k], random_state=rand_vals[i])\n",
    "        X_ros, Y_ros = ros.fit_sample(X,y)\n",
    "        lgr= LogisticRegression(C=1e+15, tol=1e-8, max_iter=300, solver='lbfgs')\n",
    "        lgr.fit(X_ros, Y_ros)\n",
    "        \n",
    "        ## interesing thing is, loss calculation would run on full dataset using fitting model of sample data. so prediction would happen on full dataset.\n",
    "        \n",
    "        loss, conf_mat = loss_cal(lgr, X, y, fp_cost, fn_cost)\n",
    "        fn_loss[i] = loss[0]\n",
    "        fp_loss[i] = loss[1]\n",
    "        misc[i] = conf_mat[1]+conf_mat[2]\n",
    "        print(conf_mat)\n",
    "        \n",
    "# calculate an average of missclassfication rate across all samples for one ratio type\n",
    "\n",
    "    misc = np.sum(misc)/(10500 * len(rand_vals)) # Why are we dividing by 10500? We should divide by total sample size of 10 samples \n",
    "\n",
    "    fn_avg_loss = np.average(fn_loss)\n",
    "    fp_avg_loss = np.average(fp_loss)\n",
    "\n",
    "    total_loss = fn_loss + fp_loss\n",
    "\n",
    "    avg_loss = np.average(total_loss)\n",
    "    std_loss = np.std(total_loss)\n",
    "\n",
    "    print(\"{:.<23s}{:10.4f}\".format(\"Misclassification Rate\", misc))\n",
    "    print(\"{:.<23s}{:10.0f}\".format(\"False Negative Cost\", fn_avg_loss))\n",
    "    print(\"{:.<23s}{:10.0f}\".format(\"False Positive Cost\", fp_avg_loss))\n",
    "    print(\"{:.<23s}{:10.0f}{:5s}{:<10.2f}\".format(\"Total Loss\", avg_loss, \" +/- \", std_loss))\n",
    "\n",
    "    # set a higher value for min_loss so that the function doesnt output error if min loss is higher than that\n",
    "\n",
    "    if avg_loss < min_loss:\n",
    "        min_loss = avg_loss\n",
    "        best_ratio = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")## Using SMOTE Tomek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Model using 0.10 ROS\n",
      "[58, 442, 44, 9956]\n",
      "[17, 483, 33, 9967]\n",
      "[58, 442, 79, 9921]\n",
      "[54, 446, 54, 9946]\n",
      "[57, 443, 54, 9946]\n",
      "[59, 441, 61, 9939]\n",
      "[78, 422, 76, 9924]\n",
      "[63, 437, 76, 9924]\n",
      "[76, 424, 89, 9911]\n",
      "[71, 429, 52, 9948]\n",
      "Counter({1.0: 10000, 0.0: 1000})\n",
      "Misclassification Rate.    0.0479\n",
      "False Negative Cost....     49325\n",
      "False Positive Cost....   1652021\n",
      "Total Loss.............   1701346 +/- 108866.25 \n",
      "\n",
      "Logistic Regression Model using 0.20 ROS\n",
      "[173, 327, 381, 9619]\n",
      "[121, 379, 279, 9721]\n",
      "[139, 361, 317, 9683]\n",
      "[158, 342, 375, 9625]\n",
      "[158, 342, 362, 9638]\n",
      "[149, 351, 354, 9646]\n",
      "[139, 361, 275, 9725]\n",
      "[127, 373, 278, 9722]\n",
      "[161, 339, 321, 9679]\n",
      "[126, 374, 289, 9711]\n",
      "Counter({1.0: 10000, 0.0: 2000})\n",
      "Misclassification Rate.    0.0646\n",
      "False Negative Cost....    161796\n",
      "False Positive Cost....   1293566\n",
      "Total Loss.............   1455363 +/- 54481.15  \n",
      "\n",
      "Logistic Regression Model using 0.30 ROS\n",
      "[220, 280, 700, 9300]\n",
      "[171, 329, 676, 9324]\n",
      "[226, 274, 711, 9289]\n",
      "[213, 287, 735, 9265]\n",
      "[216, 284, 732, 9268]\n",
      "[212, 288, 720, 9280]\n",
      "[216, 284, 688, 9312]\n",
      "[195, 305, 673, 9327]\n",
      "[212, 288, 730, 9270]\n",
      "[215, 285, 643, 9357]\n",
      "Counter({1.0: 10000, 0.0: 3000})\n",
      "Misclassification Rate.    0.0944\n",
      "False Negative Cost....    313236\n",
      "False Positive Cost....   1021334\n",
      "Total Loss.............   1334569 +/- 44552.78  \n",
      "\n",
      "Logistic Regression Model using 0.40 ROS\n",
      "[250, 250, 1143, 8857]\n",
      "[239, 261, 1024, 8976]\n",
      "[256, 244, 970, 9030]\n",
      "[253, 247, 1077, 8923]\n",
      "[250, 250, 1118, 8882]\n",
      "[239, 261, 982, 9018]\n",
      "[249, 251, 1024, 8976]\n",
      "[223, 277, 1197, 8803]\n",
      "[255, 245, 1048, 8952]\n",
      "[254, 246, 1091, 8909]\n",
      "Counter({1.0: 10000, 0.0: 4000})\n",
      "Misclassification Rate.    0.1258\n",
      "False Negative Cost....    456276\n",
      "False Positive Cost....    865553\n",
      "Total Loss.............   1321829 +/- 61058.88  \n",
      "\n",
      "Logistic Regression Model using 0.50 ROS\n",
      "[283, 217, 1422, 8578]\n",
      "[272, 228, 1460, 8540]\n",
      "[268, 232, 1394, 8606]\n",
      "[267, 233, 1414, 8586]\n",
      "[280, 220, 1430, 8570]\n",
      "[282, 218, 1376, 8624]\n",
      "[280, 220, 1384, 8616]\n",
      "[270, 230, 1303, 8697]\n",
      "[267, 233, 1486, 8514]\n",
      "[261, 239, 1425, 8575]\n",
      "Counter({1.0: 10000, 0.0: 5000})\n",
      "Misclassification Rate.    0.1558\n",
      "False Negative Cost....    569450\n",
      "False Positive Cost....    747934\n",
      "Total Loss.............   1317384 +/- 42458.79  \n",
      "\n",
      "Logistic Regression Model using 0.60 ROS\n",
      "[297, 203, 1644, 8356]\n",
      "[303, 197, 1601, 8399]\n",
      "[292, 208, 1671, 8329]\n",
      "[293, 207, 1676, 8324]\n",
      "[297, 203, 1505, 8495]\n",
      "[313, 187, 1524, 8476]\n",
      "[293, 207, 1686, 8314]\n",
      "[292, 208, 1725, 8275]\n",
      "[299, 201, 1649, 8351]\n",
      "[299, 201, 1685, 8315]\n",
      "Counter({1.0: 10000, 0.0: 6000})\n",
      "Misclassification Rate.    0.1751\n",
      "False Negative Cost....    656343\n",
      "False Positive Cost....    618579\n",
      "Total Loss.............   1274922 +/- 44367.72  \n",
      "\n",
      "Logistic Regression Model using 0.70 ROS\n",
      "[311, 189, 1907, 8093]\n",
      "[303, 197, 1925, 8075]\n",
      "[315, 185, 1934, 8066]\n",
      "[323, 177, 1970, 8030]\n",
      "[309, 191, 1900, 8100]\n",
      "[318, 182, 1910, 8090]\n",
      "[318, 182, 1958, 8042]\n",
      "[327, 173, 1941, 8059]\n",
      "[317, 183, 1947, 8053]\n",
      "[316, 184, 1894, 8106]\n",
      "Counter({1.0: 10000, 0.0: 7000})\n",
      "Misclassification Rate.    0.2012\n",
      "False Negative Cost....    753593\n",
      "False Positive Cost....    556437\n",
      "Total Loss.............   1310030 +/- 33073.74  \n",
      "\n",
      "Logistic Regression Model using 0.80 ROS\n",
      "[330, 170, 2126, 7874]\n",
      "[338, 162, 2097, 7903]\n",
      "[344, 156, 2152, 7848]\n",
      "[328, 172, 2094, 7906]\n",
      "[337, 163, 2292, 7708]\n",
      "[325, 175, 2089, 7911]\n",
      "[336, 164, 2100, 7900]\n",
      "[339, 161, 2155, 7845]\n",
      "[335, 165, 2095, 7905]\n",
      "[325, 175, 2062, 7938]\n",
      "Counter({1.0: 10000, 0.0: 8000})\n",
      "Misclassification Rate.    0.2183\n",
      "False Negative Cost....    820733\n",
      "False Positive Cost....    500686\n",
      "Total Loss.............   1321419 +/- 25634.31  \n",
      "\n",
      "Logistic Regression Model using 0.90 ROS\n",
      "[348, 152, 2412, 7588]\n",
      "[358, 142, 2367, 7633]\n",
      "[352, 148, 2390, 7610]\n",
      "[361, 139, 2410, 7590]\n",
      "[366, 134, 2394, 7606]\n",
      "[358, 142, 2432, 7568]\n",
      "[348, 152, 2327, 7673]\n",
      "[349, 151, 2356, 7644]\n",
      "[350, 150, 2376, 7624]\n",
      "[350, 150, 2319, 7681]\n",
      "Counter({1.0: 10000, 0.0: 9000})\n",
      "Misclassification Rate.    0.2404\n",
      "False Negative Cost....    883548\n",
      "False Positive Cost....    452144\n",
      "Total Loss.............   1335692 +/- 16514.02  \n",
      "\n",
      "Logistic Regression Model using 1.00 ROS\n",
      "[357, 143, 2603, 7397]\n",
      "[368, 132, 2544, 7456]\n",
      "[370, 130, 2549, 7451]\n",
      "[366, 134, 2554, 7446]\n",
      "[365, 135, 2507, 7493]\n",
      "[358, 142, 2497, 7503]\n",
      "[371, 129, 2582, 7418]\n",
      "[363, 137, 2501, 7499]\n",
      "[361, 139, 2699, 7301]\n",
      "[362, 138, 2658, 7342]\n",
      "Counter({1.0: 10000, 0.0: 10000})\n",
      "Misclassification Rate.    0.2576\n",
      "False Negative Cost....    946512\n",
      "False Positive Cost....    419499\n",
      "Total Loss.............   1366011 +/- 24114.37  \n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTETomek, SMOTEENN\n",
    "from collections import Counter\n",
    "\n",
    "# Using SMOTE--\n",
    "# Ratio of Good vs Bad in ratio list. Dictionary has Bad: Good values\n",
    "ratio = np.linspace(0.1, 1.0, 10)\n",
    "# n_majority = ratio x n_minority\n",
    "# rus_ratio = ({0:10000, 1:10000}, {0:6666, 1:10000}, {0:4286, 1:10000}, {0:2500, 1:10000},\\\n",
    "#              {0:1111,  1:10000})\n",
    "\n",
    "# Create 10 different samples for each ratio.\n",
    "# Create random seeds\n",
    "rand_val = np.array([1, 12, 123, 1234, 12345, 654321, 54321, 4321, 321, 21])\n",
    "\n",
    "# Best model is one that minimizes the loss\n",
    "min_loss   = 9e+15\n",
    "best_ratio = 0\n",
    "# \n",
    "for k in  range(len(ratio)):\n",
    "    rand_vals = (k+1)*rand_val\n",
    "    print(\"\\nLogistic Regression Model using \" + format(ratio[k], '0.2f') + \" ROS\")\n",
    "    fn_loss = np.zeros(len(rand_vals))\n",
    "    fp_loss = np.zeros(len(rand_vals))\n",
    "    misc    = np.zeros(len(rand_vals))\n",
    "    for i in range(len(rand_vals)):\n",
    "        ros = SMOTETomek(ratio=ratio[k], random_state=rand_vals[i])\n",
    "        X_ros, Y_ros = ros.fit_sample(X,y)\n",
    "        lgr= LogisticRegression(C=1e+15, tol=1e-8, max_iter=300, solver='lbfgs')\n",
    "        lgr.fit(X_ros, Y_ros)\n",
    "        \n",
    "        ## interesing thing is, loss calculation would run on full dataset using fitting model of sample data. so prediction would happen on full dataset.\n",
    "        \n",
    "        loss, conf_mat = loss_cal(lgr, X, y, fp_cost, fn_cost)\n",
    "        fn_loss[i] = loss[0]\n",
    "        fp_loss[i] = loss[1]\n",
    "        misc[i] = conf_mat[1]+conf_mat[2]\n",
    "        print(conf_mat)\n",
    "        \n",
    "        \n",
    "# calculate an average of missclassfication rate across all samples for one ratio type\n",
    "\n",
    "    misc = np.sum(misc)/(10500 * len(rand_vals)) # Why are we dividing by 10500? We should divide by total sample size of 10 samples \n",
    "\n",
    "    fn_avg_loss = np.average(fn_loss)\n",
    "    fp_avg_loss = np.average(fp_loss)\n",
    "\n",
    "    total_loss = fn_loss + fp_loss\n",
    "\n",
    "    avg_loss = np.average(total_loss)\n",
    "    std_loss = np.std(total_loss)\n",
    "    print(Counter(Y_ros))\n",
    "    print(\"{:.<23s}{:10.4f}\".format(\"Misclassification Rate\", misc))\n",
    "    print(\"{:.<23s}{:10.0f}\".format(\"False Negative Cost\", fn_avg_loss))\n",
    "    print(\"{:.<23s}{:10.0f}\".format(\"False Positive Cost\", fp_avg_loss))\n",
    "    print(\"{:.<23s}{:10.0f}{:5s}{:<10.2f}\".format(\"Total Loss\", avg_loss, \" +/- \", std_loss))\n",
    "\n",
    "    # set a higher value for min_loss so that the function doesnt output error if min loss is higher than that\n",
    "\n",
    "    if avg_loss < min_loss:\n",
    "        min_loss = avg_loss\n",
    "        best_ratio = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
